# Copyright (c) OpenMMLab. All rights reserved.

from typing import Any, Dict, Optional, Sequence, Tuple, Union

import mmcv
import numpy as np
import torch
from mmcv.parallel import collate
from torch.utils.data import Dataset

from mmdeploy.codebase.base import BaseTask
from mmdeploy.codebase.mmpose.deploy.mmpose import MMPOSE_TASK
from mmdeploy.utils import Task


@MMPOSE_TASK.register_module(Task.POSE_DETECTION.value)
class PoseDetection(BaseTask):

    def __init__(self, model_cfg: mmcv.Config, deploy_cfg: mmcv.Config,
                 device: str) -> None:
        super().__init__(model_cfg, deploy_cfg, device)

    def init_backend_model(self,
                           model_files: Sequence[str] = None,
                           **kwargs) -> torch.nn.Module:
        """Initialize backend model.

        Args:
            model_files (Sequence[str]): Input model files. Default is None.

        Returns:
            nn.Module: An initialized backend model.
        """
        from .pose_detection_model import build_pose_detection_model
        model = build_pose_detection_model(
            model_files, self.model_cfg, self.deploy_cfg, device=self.device)
        return model.eval()

    def init_pytorch_model(self,
                           model_checkpoint: Optional[str] = None,
                           **kwargs) -> torch.nn.Module:
        """Initialize torch model.

        Args:
            model_checkpoint (str): The checkpoint file of torch model,
                defaults to `None`.
            cfg_options (dict): Optional config key-pair parameters.

        Returns:
            nn.Module: An initialized torch model generated by other OpenMMLab
                codebases.
        """
        from mmpose.apis import init_pose_model
        from mmcv.cnn.utils import revert_sync_batchnorm
        model = init_pose_model(self.model_cfg, model_checkpoint, self.device)
        model = revert_sync_batchnorm(model)
        return model

    def create_input(self,
                     imgs: Union[str, np.ndarray],
                     input_shape: Sequence[int] = None,
                     **kwargs) -> Tuple[Dict, torch.Tensor]:
        from mmpose.datasets.pipelines import Compose
        from mmpose.datasets.dataset_info import DatasetInfo
        from mmpose.apis.inference import LoadImage, _box2cs
        from PIL import Image

        cfg = self.model_cfg

        dataset_info = cfg.data.test.dataset_info
        dataset_info = DatasetInfo(dataset_info)

        # create dummy person results
        if isinstance(imgs, str):
            width, height = Image.open(imgs).size
        else:
            height, width = imgs.shape[:2]
        person_results = [{'bbox': np.array([0, 0, width, height])}]
        bboxes = np.array([box['bbox'] for box in person_results])

        # build the data pipeline
        channel_order = cfg.test_pipeline[0].get('channel_order', 'rgb')
        test_pipeline = [LoadImage(channel_order=channel_order)
                         ] + cfg.test_pipeline[1:]
        test_pipeline = Compose(test_pipeline)
        dataset_name = dataset_info.dataset_name
        flip_pairs = dataset_info.flip_pairs
        batch_data = []
        for bbox in bboxes:
            center, scale = _box2cs(cfg, bbox)

            # prepare data
            data = {
                'img_or_path':
                imgs,
                'center':
                center,
                'scale':
                scale,
                'bbox_score':
                bbox[4] if len(bbox) == 5 else 1,
                'bbox_id':
                0,  # need to be assigned if batch_size > 1
                'dataset':
                dataset_name,
                'joints_3d':
                np.zeros((cfg.data_cfg.num_joints, 3), dtype=np.float32),
                'joints_3d_visible':
                np.zeros((cfg.data_cfg.num_joints, 3), dtype=np.float32),
                'rotation':
                0,
                'ann_info': {
                    'image_size': np.array(cfg.data_cfg['image_size']),
                    'num_joints': cfg.data_cfg['num_joints'],
                    'flip_pairs': flip_pairs
                }
            }
            data = test_pipeline(data)
            batch_data.append(data)

        batch_data = collate(batch_data, samples_per_gpu=1)
        # batch_data['img'] = batch_data['img'].to(self.device)
        # get all img_metas of each bounding box
        batch_data['img_metas'] = [
            img_metas[0] for img_metas in batch_data['img_metas'].data
        ]
        return batch_data, batch_data['img']

    def visualize(
        self,
        model: torch.nn.Module,
        image: Union[str, np.ndarray],
        result: list,
        output_file: str,
        window_name: str,
        show_result: bool = False,
        radius: int = 4,
        thickness: int = 1,
    ):
        from mmpose.datasets.dataset_info import DatasetInfo
        dataset_info = self.model_cfg.data.test.dataset_info
        dataset_info = DatasetInfo(dataset_info)
        skeleton = dataset_info.skeleton
        pose_kpt_color = dataset_info.pose_kpt_color
        pose_link_color = dataset_info.pose_link_color
        if hasattr(model, 'module'):
            model = model.module
        if isinstance(image, str):
            image = mmcv.imread(image)
        result = [dict(keypoints=pose) for pose in result['preds']]
        model.show_result(
            image,
            result,
            skeleton=skeleton,
            radius=radius,
            thickness=thickness,
            pose_kpt_color=pose_kpt_color,
            pose_link_color=pose_link_color,
            out_file=output_file,
            show=show_result,
            win_name=window_name)

    def evaluate_outputs(self,
                         model_cfg,
                         outputs: Sequence,
                         dataset: Dataset,
                         metrics: Optional[str] = None,
                         out: Optional[str] = None,
                         metric_options: Optional[dict] = None,
                         format_only: bool = False,
                         **kwargs):
        pass

    def get_model_name(self) -> str:
        pass

    def get_partition_cfg(self, partition_type: str, **kwargs) -> Dict:
        raise NotImplementedError('Not supported yet.')

    def get_preprocess(self) -> Dict:
        pass

    def get_postprocess(self) -> Dict:
        pass

    def get_tensor_from_input(self, input_data: Dict[str, Any],
                              **kwargs) -> torch.Tensor:
        img = input_data['img']
        if isinstance(img, (list, tuple)):
            img = img[0]
        return img

    def run_inference(self, model, model_inputs: Dict[str, torch.Tensor]):
        output = model(
            **model_inputs,
            return_loss=False,
            return_heatmap=False,
            target=None,
            target_weight=None)
        return [output]
